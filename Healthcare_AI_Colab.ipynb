{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Healthcare-AI Deployment Notebook\n",
                "Run this notebook in Google Colab to deploy the AI Agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28891a23",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 0. Check GPU Status\n",
                "!nvidia-smi\n",
                "# If this fails, go to Runtime -> Change runtime type -> T4 GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "18456be8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install System Dependencies & Ollama\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!apt-get install -y libduckdb-dev # Optional depending on duckdb wheel availability\n",
                "\n",
                "# Install lshw to help Ollama detect GPU if needed\n",
                "!apt-get install -y pciutils lshw"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0abe6fa8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Start Ollama Server in Background\n",
                "import subprocess\n",
                "import time\n",
                "import urllib.request\n",
                "import socket\n",
                "import os\n",
                "\n",
                "# Set LD_LIBRARY_PATH to help Ollama find NVIDIA libraries\n",
                "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/lib64-nvidia:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\n",
                "\n",
                "# Start Ollama and redirect logs to a file for debugging\n",
                "with open(\"ollama.log\", \"w\") as log_file:\n",
                "    process = subprocess.Popen(['ollama', 'serve'], stdout=log_file, stderr=subprocess.STDOUT)\n",
                "\n",
                "print(\"Starting Ollama...\")\n",
                "\n",
                "# Poll for service availability\n",
                "start_time = time.time()\n",
                "while time.time() - start_time < 60:\n",
                "    try:\n",
                "        with urllib.request.urlopen(\"http://localhost:11434\") as response:\n",
                "            if response.status == 200:\n",
                "                print(\"Ollama is running!\")\n",
                "                break\n",
                "    except (urllib.error.URLError, ConnectionRefusedError, socket.error):\n",
                "        time.sleep(1)\n",
                "else:\n",
                "    print(\"Error: Ollama failed to start within 60 seconds.\")\n",
                "    print(\"--- Ollama Logs ---\")\n",
                "    with open(\"ollama.log\", \"r\") as f:\n",
                "        print(f.read())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d68aebe8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Pull Typhoon Models (This may take a few minutes)\n",
                "!ollama pull scb10x/llama3.1-typhoon2-8b-instruct:latest\n",
                "!ollama pull scb10x/llama3.2-typhoon2-1b-instruct:latest\n",
                "!ollama list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da12116d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Fetch Code from GitHub (CI/CD)\n",
                "import os\n",
                "\n",
                "REPO_URL = \"https://github.com/pongchanan/Healthcare-AI.git\"\n",
                "PROJECT_DIR = \"Healthcare-AI\"\n",
                "\n",
                "if os.path.exists(PROJECT_DIR):\n",
                "    print(f\"Updating existing repository in {PROJECT_DIR}...\")\n",
                "    %cd {PROJECT_DIR}\n",
                "    !git pull\n",
                "    %cd ..\n",
                "else:\n",
                "    print(f\"Cloning repository from {REPO_URL}...\")\n",
                "    !git clone {REPO_URL}\n",
                "\n",
                "print(\"âœ… Code synced successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8691fdbf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Install Python Dependencies\n",
                "%cd Healthcare-AI\n",
                "!pip install -r requirements.txt\n",
                "!pip install pyngrok # For exposing the server"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3dfa3a50",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Run Ingestion (Build Databases)\n",
                "!python -m src.pipelines.sql_loader\n",
                "!python -m src.pipelines.ingestion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4aec9e2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Start the API with Ngrok Public URL\n",
                "from pyngrok import ngrok\n",
                "import nest_asyncio\n",
                "import uvicorn\n",
                "from main import app\n",
                "\n",
                "# Authenticate ngrok (Optional but recommended for stability - get token from ngrok.com)\n",
                "ngrok.set_auth_token(\"33KFiXOGZWUksDWun4lAmtyfPE6_DCTBB1Dq4f2pwiZtfNXz\")\n",
                "\n",
                "public_url = ngrok.connect(8000)\n",
                "print(f\"\\nðŸ”¥ Public API URL: {public_url}\\n\")\n",
                "\n",
                "nest_asyncio.apply()\n",
                "# uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
                "\n",
                "# In Colab/Jupyter, we must run uvicorn asynchronously to avoid \"asyncio.run() cannot be called from a running event loop\"\n",
                "config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
                "server = uvicorn.Server(config)\n",
                "await server.serve()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2052a45d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# DEBUG: Check Ollama Logs if issues persist\n",
                "!cat /content/ollama.log"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
